{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyAPrVeCl8gc"
      },
      "source": [
        "# QCTO - Workplace Module\n",
        "\n",
        "### Mastering Avocado Pricing: A Strategic Approach to Regional, Seasonal, and Predictive Insights\n",
        "#### Done By: Keneilwe Rangwaga\n",
        "\n",
        "© ExploreAI 2024\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://plantsvibe.com/wp-content/uploads/2024/05/avocado-tree-growth-phases.jpg\" alt=\"Navigating Avocado Pricing\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"toc\"></a>\n",
        "## Table of Contents\n",
        "\n",
        "<a href=#BC> Background Context</a>\n",
        "\n",
        "<a href=#one>1. Importing Packages</a>\n",
        "\n",
        "<a href=#two>2. Data Collection and Description</a>\n",
        "\n",
        "<a href=#three>3. Loading Data </a>\n",
        "\n",
        "<a href=#four>4. Data Cleaning and Filtering</a>\n",
        "\n",
        "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
        "\n",
        "<a href=#six>6. Modeling </a>\n",
        "\n",
        "<a href=#seven>7. Evaluation and Validation</a>\n",
        "\n",
        "<a href=#eight>8. Final Model</a>\n",
        "\n",
        "<a href=#nine>9. Conclusion and Future Work</a>\n",
        "\n",
        "<a href=#ten>10. References</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp1CbaXOl8ge"
      },
      "source": [
        "---\n",
        "<a id=\"BC\"></a>\n",
        "## **Background Context**\n",
        "<a href=#toc>Back to Table of Contents</a>\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Avocados have become more than just a trendy fruit; they represent a dynamic and rapidly evolving market within the global produce industry. From guacamole at the dinner table to avocado toast on breakfast menus worldwide, the demand for avocados has surged over the past decade. However, this popularity has introduced complexities in the avocado market, particularly concerning pricing and sales volume. Various factors, including regional demand, seasonality, and market dynamics, have significantly impacted how avocados are priced and sold across different regions and times of the year.\n",
        "\n",
        "To better understand these complexities, this project aims to dissect the intricate patterns of avocado pricing and sales from 2015 to 2023, provided by the Hass Avocado Board. By exploring the interplay between geography, seasonality, and economic factors, the project aims to reveal the underlying trends and patterns that influence avocado pricing and sales. The insights gained will be crucial for stakeholders in the avocado industry to optimize their pricing strategies, forecast market trends, and enhance the overall efficiency of the supply chain.\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "Avocado pricing and sales are influenced by a complex interplay of regional differences, seasonal trends, and market dynamics. For stakeholders in the avocado industry, understanding these factors is critical to optimizing pricing strategies, forecasting future trends, and enhancing supply chain efficiency. This project seeks to address the challenge of accurately analyzing how geography, seasonality, and economic events have impacted avocado prices and sales volumes from 2015 to 2023.By developing predictive models and integrating these insights, this project aims to provide actionable recommendations that will enable better decision-making and strategic planning in the avocado market"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myAg-fkml8gf"
      },
      "source": [
        "---\n",
        "<a id=\"one\"></a>\n",
        "## **Importing Package**\n",
        "<a href=#toc>Back to Table of Contents</a>\n",
        "\n",
        "\n",
        "* In this section we're going to List and import all the Python packages that will be used throughout the project such as Pandas for data manipulation, Matplotlib/Seaborn for visualization, scikit-learn for modeling, etc.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0lGFlb1Zl8gg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')# Ignore all warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_rM-Sxl8gh"
      },
      "source": [
        "---\n",
        "<a id=\"two\"></a>\n",
        "## **Data Collection and Description**\n",
        "<a href=#toc>Back to Table of Contents</a>\n",
        "\n",
        "This data was initially downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. Here's how the Hass Avocado Board describes the data on their website:\n",
        "\n",
        "The dataset represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n",
        "\n",
        "You can access the data [here](https://www.kaggle.com/datasets/vakhariapujan/avocado-prices-and-sales-volume-2015-2023/data) through kaggle.\n",
        "\n",
        "The dataset contains 53,415 entries and 12 columns, with information on avocado prices, volumes, and regions from 2015 to 2023. Here’s a summary of the key details:\n",
        "* Categorical\n",
        "* Total Rows: 53,415\n",
        "* Total Columns: 12\n",
        "\n",
        "Columns Information:\n",
        "\n",
        "1.\tDate: The date of the observation (466 unique dates).\n",
        "2.\tAveragePrice: The average price of avocados.\n",
        "3.\tTotalVolume: The total volume of avocados sold.\n",
        "4.\tplu4046, plu4225, plu4770: Specific product look-up codes (PLU) representing different avocado products.\n",
        "\n",
        "    4046 - small/medium Hass Avocados (~3-5 oz)\n",
        "\n",
        "    4225 - large Hass Avocados (~8-10 oz)\n",
        "\n",
        "    4770 - extra large Hass Avocados (~10-15 oz)\n",
        "5.\tTotalBags, SmallBags, LargeBags, XLargeBags: Number of avocados sold in different packaging types.\n",
        "\n",
        "6.\tType: Indicates whether the avocado is “conventional” or “organic”.\n",
        "7.\tRegion: Region of the sale (60 unique regions).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6YVxdcBl8gj"
      },
      "source": [
        "---\n",
        "<a id=\"three\"></a>\n",
        "## **Loading Data**\n",
        "<a href=#toc>Back to Table of Contents</a>\n",
        "\n",
        "In this section we're going to load the data into the notebook for manipulation and analysis.\n",
        "- The code in the cell below is used to load the data/csv file and display the first few rows to give a sense of what the raw data looks like.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h674lrQCl8gj",
        "outputId": "3a8160e8-c999-422d-92d9-e8fb91d4bc42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
            "0  2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
            "1  2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
            "2  2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
            "3  2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
            "4  2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
            "\n",
            "   TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
            "0    9716.46    9186.93     529.53         0.0  conventional   \n",
            "1    1162.65    1162.65       0.00         0.0       organic   \n",
            "2   46815.79   16707.15   30108.64         0.0  conventional   \n",
            "3    1408.19    1071.35     336.84         0.0       organic   \n",
            "4  141136.68  137146.07    3990.61         0.0  conventional   \n",
            "\n",
            "                region  \n",
            "0               Albany  \n",
            "1               Albany  \n",
            "2              Atlanta  \n",
            "3              Atlanta  \n",
            "4  BaltimoreWashington  \n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Avocado_HassAvocadoBoard_20152023v1.0.1.csv')\n",
        "\n",
        "# Check the first few rows to understand the data\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCtAS7jPl8gk"
      },
      "source": [
        "---\n",
        "<a id=\"four\"></a>\n",
        "## **Data Cleaning and Filtering**\n",
        "<a href=#toc>Back to Table of Contents</a>\n",
        "\n",
        "\n",
        "Data cleaning is the essential first step in any analysis, much like laying down a strong foundation before building a house. It’s about carefully going through your dataset to spot and fix any errors, inconsistencies, or gaps, making sure everything is accurate and relevant.\n",
        "During this process, I focus on tasks like removing duplicates, correcting mistakes, and handling missing values. It’s about organizing and refining the data so that when it comes time to analyze, the results are trustworthy and precise.\n",
        "\n",
        "In short, data cleaning is crucial because it directly influences the quality of your analysis, ensuring that the insights you gain are reliable and actionable.\n",
        "\n",
        "**Below are the steps we will explore to clean the dataset**\n",
        "1. Understanding the Data\n",
        "2. Handling Missing Values\n",
        "3. Date Column Formatting\n",
        "4. Handling Inconsistent Data\n",
        "5. Feature Engineering\n",
        "6. Data Validation\n",
        "7. Final Cleaned Dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQS6x87slo4"
      },
      "source": [
        "\n",
        "\n",
        "#### 1. Understanding the Data\n",
        "\n",
        "* Check for Missing Values: Identify columns with missing values and determine the extent of missing data.\n",
        "* Check for Duplicates: Identify and remove any duplicate rows in the dataset.\n",
        "* Check Data Types: Ensure that each column has the correct data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R556td8atNuk",
        "outputId": "177d2565-2cb1-4dd1-bbcc-2689c4256182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values:\n",
            " Date                0\n",
            "AveragePrice        0\n",
            "TotalVolume         0\n",
            "plu4046             0\n",
            "plu4225             0\n",
            "plu4770             0\n",
            "TotalBags           0\n",
            "SmallBags       12390\n",
            "LargeBags       12390\n",
            "XLargeBags      12390\n",
            "type                0\n",
            "region              0\n",
            "dtype: int64\n",
            "Number of duplicate rows: 0\n",
            "Data Types:\n",
            " Date             object\n",
            "AveragePrice    float64\n",
            "TotalVolume     float64\n",
            "plu4046         float64\n",
            "plu4225         float64\n",
            "plu4770         float64\n",
            "TotalBags       float64\n",
            "SmallBags       float64\n",
            "LargeBags       float64\n",
            "XLargeBags      float64\n",
            "type             object\n",
            "region           object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Check data types\n",
        "print(\"Data Types:\\n\", df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy5OULuQteWH"
      },
      "source": [
        "#### 2. Handling Missing Values\n",
        "* Analyze Missing Data: Determine which columns have missing data and consider the best approach (e.g., imputation, deletion).  \n",
        "* Impute Missing Values: If the missing data is minimal, consider imputing it with the mean, median, or mode as appropriate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n71SbqtZuAdJ",
        "outputId": "8061e4d2-ab36-43fd-aa9f-13e338a3d837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
            "0      2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
            "1      2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
            "2      2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
            "3      2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
            "4      2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
            "...           ...           ...          ...        ...        ...       ...   \n",
            "53410  2023-12-03          1.55      5693.91     204.64    1211.25      0.00   \n",
            "53411  2023-12-03          1.70    343326.10   66808.44  132075.11     58.65   \n",
            "53412  2023-12-03          1.62     34834.86   15182.42    1211.38      0.00   \n",
            "53413  2023-12-03          1.25      2942.83    1058.54       7.46      0.00   \n",
            "53414  2023-12-03          1.48   2010020.72  271808.32  274480.64     63.43   \n",
            "\n",
            "        TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
            "0         9716.46    9186.93     529.53        0.00  conventional   \n",
            "1         1162.65    1162.65       0.00        0.00       organic   \n",
            "2        46815.79   16707.15   30108.64        0.00  conventional   \n",
            "3         1408.19    1071.35     336.84        0.00       organic   \n",
            "4       141136.68  137146.07    3990.61        0.00  conventional   \n",
            "...           ...        ...        ...         ...           ...   \n",
            "53410     4278.03  103922.17   23313.16     2731.81       organic   \n",
            "53411   138830.45  103922.17   23313.16     2731.81       organic   \n",
            "53412    18075.66  103922.17   23313.16     2731.81       organic   \n",
            "53413     1779.19  103922.17   23313.16     2731.81       organic   \n",
            "53414  1364514.02  103922.17   23313.16     2731.81       organic   \n",
            "\n",
            "                    region  \n",
            "0                   Albany  \n",
            "1                   Albany  \n",
            "2                  Atlanta  \n",
            "3                  Atlanta  \n",
            "4      BaltimoreWashington  \n",
            "...                    ...  \n",
            "53410               Toledo  \n",
            "53411                 West  \n",
            "53412     WestTexNewMexico  \n",
            "53413              Wichita  \n",
            "53414              TotalUS  \n",
            "\n",
            "[53415 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "# Impute missing values with the mean\n",
        "df['SmallBags'].fillna(df['SmallBags'].mean(), inplace=True)\n",
        "df['LargeBags'].fillna(df['LargeBags'].mean(), inplace=True)\n",
        "df['XLargeBags'].fillna(df['XLargeBags'].mean(), inplace=True)\n",
        "\n",
        "# Format the values to 2 decimal places\n",
        "df = df.round(2)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLjeXYjsudyq"
      },
      "source": [
        "#### 3. Date Column Formatting\n",
        "* Convert Date Column: Ensure the Date column is in datetime format to allow for easier manipulation and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQe65NHfuhM5",
        "outputId": "c4434178-4fa1-4d6b-ad7d-2dfe0465793d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
            "0     2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
            "1     2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
            "2     2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
            "3     2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
            "4     2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
            "...          ...           ...          ...        ...        ...       ...   \n",
            "53410 2023-12-03          1.55      5693.91     204.64    1211.25      0.00   \n",
            "53411 2023-12-03          1.70    343326.10   66808.44  132075.11     58.65   \n",
            "53412 2023-12-03          1.62     34834.86   15182.42    1211.38      0.00   \n",
            "53413 2023-12-03          1.25      2942.83    1058.54       7.46      0.00   \n",
            "53414 2023-12-03          1.48   2010020.72  271808.32  274480.64     63.43   \n",
            "\n",
            "        TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
            "0         9716.46    9186.93     529.53        0.00  conventional   \n",
            "1         1162.65    1162.65       0.00        0.00       organic   \n",
            "2        46815.79   16707.15   30108.64        0.00  conventional   \n",
            "3         1408.19    1071.35     336.84        0.00       organic   \n",
            "4       141136.68  137146.07    3990.61        0.00  conventional   \n",
            "...           ...        ...        ...         ...           ...   \n",
            "53410     4278.03  103922.17   23313.16     2731.81       organic   \n",
            "53411   138830.45  103922.17   23313.16     2731.81       organic   \n",
            "53412    18075.66  103922.17   23313.16     2731.81       organic   \n",
            "53413     1779.19  103922.17   23313.16     2731.81       organic   \n",
            "53414  1364514.02  103922.17   23313.16     2731.81       organic   \n",
            "\n",
            "                    region  \n",
            "0                   Albany  \n",
            "1                   Albany  \n",
            "2                  Atlanta  \n",
            "3                  Atlanta  \n",
            "4      BaltimoreWashington  \n",
            "...                    ...  \n",
            "53410               Toledo  \n",
            "53411                 West  \n",
            "53412     WestTexNewMexico  \n",
            "53413              Wichita  \n",
            "53414              TotalUS  \n",
            "\n",
            "[53415 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "# Convert the 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsXTJCxfuhnU"
      },
      "source": [
        "#### 4. Handling Inconsistent Data\n",
        "* Standardize Text Data: Ensure consistency in text-based columns, make sure there is no inconsistent capitalization or spelling.\n",
        "* Correct Outliers: Identify any outliers that may affect the analysis, especially in price or volume-related columns. Outliers should be analyzed to decide if they should be removed or retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pXEk_2IiukxO"
      },
      "outputs": [],
      "source": [
        "# Standardize text data\n",
        "df['region'] = df['region'].str.lower().str.strip()# Convert to lowercase and strip leading/trailing spaces\n",
        "df['type'] = df['type'].str.lower().str.strip()  # Convert to lowercase and strip leading/trailing spaces\n",
        "df['region'] = df['region'].str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n",
        "df['type'] = df['type'].str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n",
        "\n",
        "# Detect outliers using Z-score\n",
        "# Calculate Z-scores for the 'AveragePrice' column\n",
        "df['z_score'] = np.abs(stats.zscore(df['AveragePrice']))\n",
        "\n",
        "# Filter out outliers (Z-score > 3 is often considered an outlier)\n",
        "df_clean = df[df['z_score'] < 3]\n",
        "df_clean.drop(columns=['z_score'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQb5R8KSu8oH"
      },
      "source": [
        "#### 5. Feature Engineering\n",
        "\n",
        "\n",
        "*\tAggregate Data: Depending on the analysis goals, consider aggregating data by region, year, or quarter.\n",
        "*   Create New Features: Based on existing columns, create new features that may be useful for analysis\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJarw73ZvACN",
        "outputId": "a8177f8d-cbbf-4467-c75f-fe3612596ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
            "0     2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
            "1     2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
            "2     2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
            "3     2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
            "4     2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
            "...          ...           ...          ...        ...        ...       ...   \n",
            "53410 2023-12-03          1.55      5693.91     204.64    1211.25      0.00   \n",
            "53411 2023-12-03          1.70    343326.10   66808.44  132075.11     58.65   \n",
            "53412 2023-12-03          1.62     34834.86   15182.42    1211.38      0.00   \n",
            "53413 2023-12-03          1.25      2942.83    1058.54       7.46      0.00   \n",
            "53414 2023-12-03          1.48   2010020.72  271808.32  274480.64     63.43   \n",
            "\n",
            "        TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
            "0         9716.46    9186.93     529.53        0.00  conventional   \n",
            "1         1162.65    1162.65       0.00        0.00       organic   \n",
            "2        46815.79   16707.15   30108.64        0.00  conventional   \n",
            "3         1408.19    1071.35     336.84        0.00       organic   \n",
            "4       141136.68  137146.07    3990.61        0.00  conventional   \n",
            "...           ...        ...        ...         ...           ...   \n",
            "53410     4278.03  103922.17   23313.16     2731.81       organic   \n",
            "53411   138830.45  103922.17   23313.16     2731.81       organic   \n",
            "53412    18075.66  103922.17   23313.16     2731.81       organic   \n",
            "53413     1779.19  103922.17   23313.16     2731.81       organic   \n",
            "53414  1364514.02  103922.17   23313.16     2731.81       organic   \n",
            "\n",
            "                    region  Year  Month  PricePerUnitVolume  \n",
            "0                   albany  2015      1        2.984835e-05  \n",
            "1                   albany  2015      1        1.302813e-03  \n",
            "2                  atlanta  2015      1        2.298737e-06  \n",
            "3                  atlanta  2015      1        4.575362e-04  \n",
            "4      baltimorewashington  2015      1        1.370515e-06  \n",
            "...                    ...   ...    ...                 ...  \n",
            "53410               toledo  2023     12        2.722207e-04  \n",
            "53411                 west  2023     12        4.951561e-06  \n",
            "53412     westtexnewmexico  2023     12        4.650514e-05  \n",
            "53413              wichita  2023     12        4.247612e-04  \n",
            "53414              totalus  2023     12        7.363108e-07  \n",
            "\n",
            "[53162 rows x 15 columns]\n"
          ]
        }
      ],
      "source": [
        "# Extract year, month, and day features\n",
        "df_clean['Year'] = df_clean['Date'].dt.year\n",
        "df_clean['Month'] = df_clean['Date'].dt.month\n",
        "\n",
        "# Create a new feature 'PricePerUnitVolume'\n",
        "# Calculate the price per unit volume (AveragePrice / TotalVolume)\n",
        "df_clean['PricePerUnitVolume'] = df_clean['AveragePrice'] / df_clean['TotalVolume']\n",
        "\n",
        "# Aggregate data by region and year\n",
        "agg_data = df_clean.groupby(['region', 'Year']).agg({\n",
        "    'AveragePrice': 'mean',\n",
        "    'TotalVolume': 'sum'\n",
        "}).reset_index()\n",
        "print(df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vISmrNsvO7D"
      },
      "source": [
        "#### 6. Data Validation\n",
        "\n",
        "* Verify Data Consistency: Check for any inconsistencies or anomalies across the dataset after cleaning.\n",
        "* Cross-Validation: Validate the cleaned data by cross-checking with any available external sources or by ensuring logical consistency across different columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q0G_bxQvPjQ",
        "outputId": "1dd00afd-ae16-440a-c1e7-7d441646af16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                Date  AveragePrice   TotalVolume  \\\n",
            "count                          53162  53162.000000  5.316200e+04   \n",
            "mean   2019-07-21 23:22:22.567999744      1.422541  8.735184e+05   \n",
            "min              2015-01-04 00:00:00      0.440000  8.456000e+01   \n",
            "25%              2017-05-14 00:00:00      1.120000  1.647635e+04   \n",
            "50%              2019-08-11 00:00:00      1.400000  1.217986e+05   \n",
            "75%              2021-10-03 00:00:00      1.690000  4.571965e+05   \n",
            "max              2023-12-03 00:00:00      2.600000  6.103446e+07   \n",
            "std                              NaN      0.382925  3.553208e+06   \n",
            "\n",
            "            plu4046       plu4225       plu4770     TotalBags     SmallBags  \\\n",
            "count  5.316200e+04  5.316200e+04  5.316200e+04  5.316200e+04  5.316200e+04   \n",
            "mean   2.996787e+05  2.232417e+05  2.062959e+04  2.185275e+05  1.042667e+05   \n",
            "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
            "25%    7.079475e+02  2.118660e+03  0.000000e+00  8.010360e+03  0.000000e+00   \n",
            "50%    1.493233e+04  1.780699e+04  9.240000e+01  3.732899e+04  1.164922e+04   \n",
            "75%    1.297281e+05  9.440106e+04  3.660605e+03  1.117851e+05  1.039222e+05   \n",
            "max    2.544720e+07  2.047057e+07  2.860025e+06  1.629830e+07  1.256716e+07   \n",
            "std    1.310618e+06  9.576174e+05  1.043355e+05  8.696309e+05  5.000380e+05   \n",
            "\n",
            "          LargeBags     XLargeBags          Year         Month  \\\n",
            "count  5.316200e+04   53162.000000  53162.000000  53162.000000   \n",
            "mean   2.338602e+04    2741.001222   2019.059272      6.462605   \n",
            "min    0.000000e+00       0.000000   2015.000000      1.000000   \n",
            "25%    0.000000e+00       0.000000   2017.000000      4.000000   \n",
            "50%    5.055350e+02       0.000000   2019.000000      6.000000   \n",
            "75%    2.331316e+04    2731.810000   2021.000000      9.000000   \n",
            "max    4.324231e+06  679586.800000   2023.000000     12.000000   \n",
            "std    1.314662e+05   19843.003612      2.551390      3.434303   \n",
            "\n",
            "       PricePerUnitVolume  \n",
            "count        5.316200e+04  \n",
            "mean         1.017726e-04  \n",
            "min          1.261582e-08  \n",
            "25%          2.622613e-06  \n",
            "50%          1.059946e-05  \n",
            "75%          9.955604e-05  \n",
            "max          1.880322e-02  \n",
            "std          2.375006e-04  \n",
            "Missing Values After Cleaning:\n",
            " Date                  0\n",
            "AveragePrice          0\n",
            "TotalVolume           0\n",
            "plu4046               0\n",
            "plu4225               0\n",
            "plu4770               0\n",
            "TotalBags             0\n",
            "SmallBags             0\n",
            "LargeBags             0\n",
            "XLargeBags            0\n",
            "type                  0\n",
            "region                0\n",
            "Year                  0\n",
            "Month                 0\n",
            "PricePerUnitVolume    0\n",
            "dtype: int64\n",
            "Number of duplicate rows after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for consistency in the cleaned data\n",
        "print(df_clean.describe())\n",
        "\n",
        "# Verify that there are no missing values\n",
        "print(\"Missing Values After Cleaning:\\n\", df_clean.isnull().sum())\n",
        "\n",
        "# Check for duplicates again after cleaning\n",
        "print(f\"Number of duplicate rows after cleaning: {df_clean.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cUaLQEov5EK"
      },
      "source": [
        "#### 7. Final Cleaned Dataset\n",
        "* Create a Final Cleaned Dataset: After performing all necessary steps, create a final cleaned version of the dataset that is ready for exploratory data analysis (EDA) and modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5YKwzPWQl8gk"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned dataset\n",
        "df_clean.to_csv('cleaned_avocado_data.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
